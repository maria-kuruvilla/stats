r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
#return(Nt)
#return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
ddind.soln
# a function for caclulating negative log likelihood for density independent model
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
return(Nt)
return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
density.independent.nll(start.pars,Ntobs)
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
print(Nt)
print(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
density.independent.nll(start.pars,Ntobs)
mean(epsilon_t)
# a function for caclulating negative log likelihood for density independent model
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
print(Nt)
print(mean(epsilon_t))
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
ddind.soln
Nt[1] <- N1978
N1978 <- pars[2]
start.pars <- c(0.1, 3000, 400)
N1978 <- start.pars[2]
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
r <- start.pars[1]
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
Nt
epsilon_t <- Ntobs - Nt
epsilon_t
install.packages("xtable")
library(readr)
stats_speed <- read_csv("Documents/data/temp_collective/roi/stats_speed.csv")
View(stats_speed)
speed <- stats_speed$`90_speed`
temp <- stats_speed$Temperature
gs <- stats_speed$Groupsize
n <- length(stats_speed$`90_speed`)
speed_data <- as.data.frame(cbind(sample = (1:n),speed,temp,gs))
model <- lm(speed ~ temp + gs,speed_data)
model$coefficients
plot(fitted(model),model$residuals)
plot(fitted(model),sqrt(abs(model$residuals)))
qqnorm(residuals(model))
qqline(residuals(model))
plot.default(temp,model$residuals)
plot.default(gs,model$residuals)
plot.default(temp,model$residuals)
plot(fitted(model),model$residuals)
shapiro.test(residuals(model))
install.packages(c("faraway", "lessR"))
summary(model)
boxcox(model, plotit = TRUE)
install.packages("EnvStats")
boxcox(model, plotit = TRUE)
require(EnvStats)
boxcox(model, plotit = TRUE)
boxcox(model, plotit = TRUE)
install.packages("MASS")
require(MASS)
boxcox(model, plotit = TRUE)
boxcox(model, plotit = FALSE)
10^(0.1)
model_transform <- lm(speed^0.1 ~ temp + gs,speed_data)
summary(model_transform)
plot(fitted(model_transform), residuals(model_transform))
qqnorm(residuals(model_transform))
qqline(residuals(model_transform))
ln(1)
log(1)
log(10)
log(2.7)
model_exp <- lm(log(speed) ~ temp + gs,speed_data)
summary(model_exp)
plot(fitted(model_exp), residuals(model_exp))
qqnorm(residuals(model_exp))
qqline(residuals(model_exp))
plot(fitted(model_exp), residuals(model_exp))
plot(fitted(model), residuals(model))
shapiro.test(residuals(model_exp))
exp(2)
transform.bc <- function(x)(exp(x))
data.frame(x = temp)
data.frame(x = temp + gs)
summary(model_exp)
transform.bc <- function(x)(exp(x))
out <- predict(lmod.t, newdata = data.frame(x = temp), interval="prediction")
transform.bc <- function(x)(exp(x))
out <- predict(model_exp, newdata = data.frame(x = temp), interval="prediction")
plot(speed ~ temp, ylab = "90th percentile of speed", xlab = "Temperature")
lines(sort(temp),sort(transform.bc(out)[,1] ) ,col="red")
lines(sort(temp),sort(transform.bc(out)[,2] ),lty = 2,col="red")
plot(speed ~ temp, ylab = "90th percentile of speed", xlab = "Temperature")
lines(sort(temp),sort(transform.bc(out)[,1] ) ,col="red")
lines(sort(temp),sort(transform.bc(out)[,2] ),lty = 2,col="red")
lines(sort(temp),sort(transform.bc(out)[,3] ),lty = 2,col="red")
seq(9, 29, by=2)
data.frame(x = seq(9,29,by = 2))
transform.bc <- function(x)(exp(x))
out <- predict(model_exp, newdata = data.frame(x =seq(9,29,by = 2)), interval="prediction")
plot(speed ~ temp, ylab = "90th percentile of speed", xlab = "Temperature")
lines(sort(temp),sort(transform.bc(out)[,1] ) ,col="red")
lines(sort(temp),sort(transform.bc(out)[,2] ),lty = 2,col="red")
lines(sort(temp),sort(transform.bc(out)[,3] ),lty = 2,col="red")
lines(seq(9,29,by = 2),sort(transform.bc(out)[,1] ) ,col="red")
lines(seq(9,29,by = 2),sort(transform.bc(out)[,2] ),lty = 2,col="red")
lines(seq(9,29,by = 2),sort(transform.bc(out)[,3] ),lty = 2,col="red")
sort(transform.bc(out)[,1]
a
transform.bc(out)
out <- predict(model_exp, newdata = data.frame(x =seq(9,29,by = 2)), interval="prediction")
transform.bc(out)
out2 <- predict(model_exp, newdata = data.frame(x = gs), interval="prediction")
plot(speed ~ gs, ylab = "90th percentile of speed", xlab = "Group Size")
lines(sort(gs),sort(transform.bc(out2)[,1] ) ,col="red")
lines(sort(gs),sort(transform.bc(out2)[,2] ),lty = 2,col="red")
lines(sort(gs),sort(transform.bc(out2)[,3] ),lty = 2,col="red")
Correlation(temp,gs)
install.packages("lessR")
require(lessR)
install.packages("latticeExtra"")
a
adf
f
s
vswrdvw
dsvsvs
speeed
speed
h <- hatvalues(model_exp)
two_p_over_n <- 2*mean(h)
speed_data$sample[which(h>two_p_over_n)]
num <- speed_data$sample
halfnorm(h,nlab = 5, labs= num, ylab="Leverages")
require(faraway)
install.packages("faraway")
require(faraway)
install.packages("lme4")
library(readr)
stats_speed_acc_latency <- read_csv("Documents/data/temp_collective/roi/stats_speed_acc_latency.csv")
View(stats_speed_acc_latency)
library(readr)
stats_loom_latency <- read_csv("Documents/data/temp_collective/roi/stats_loom_latency.csv")
View(stats_loom_latency)
lat <- stats_speed_acc_latency$latency
temp <- stats_speed_acc_latency$Temperature
gs <- stats_speed_acc_latency$Groupsize
n <- length(stats_speed_acc_latency$latency)
data <- as.data.frame(cbind(sample = (1:n),temp,gs,acc,lat))
model_lat <- lm(lat ~ temp + gs,data)
summary(model_lat)
acc <- stats_speed_acc_latency$`90_acc`
data <- as.data.frame(cbind(sample = (1:n),temp,gs,acc,lat))
model_lat <- lm(lat ~ temp + gs,data)
summary(model_lat)
plot(fitted(model_lat), residuals(model_lat))
qqnorm(residuals(model_lat))
qqline(residuals(model_lat))
shapiro.test(residuals(model_lat))
model_lat_int <- lm(lat ~ temp*gs,data)
summary(model_lat_int)
plot(fitted(model_lat_int), residuals(model_lat_int))
qqnorm(residuals(model_lat_int))
qqline(residuals(model_lat_int))
shapiro.test(residuals(model_lat_int))
require(MASS)
boxcox(model_lat, lambda = seq(-10, 0, 1/10),plotit=TRUE)
model_quad <- lm(lat ~ temp + gs + I(temp^2),data)
summary(model_quad)
plot(fitted(model_quad), residuals(model_quad))
qqnorm(residuals(model_quad))
qqline(residuals(model_quad))
shapiro.test(residuals(model_quad))
model_quad_int <- lm(lat ~ temp*gs + I(temp^2),data)
summary(model_quad_int)
plot(fitted(model_quad_int), residuals(model_quad_int))
qqnorm(residuals(model_quad_int))
qqline(residuals(model_quad_int))
shapiro.test(residuals(model_quad_int))
model_pois <- glm(lat ~ temp + gs, family = poisson,stats_speed_acc_latency)
summary(model_pois)
model_glm <- glm.nb(lat ~ gs*temp, data)
summary(model_glm)
model_pois_quad <- glm(lat ~ temp + gs + I(temp^2), family = poisson,stats_speed_acc_latency)
summary(model_pois_quad)
model_quad_int <- lm(lat ~ temp*gs + I(temp^2),data)
summary(model_quad_int)
library(readr)
stats_loom_latency_nan <- read_csv("Documents/data/temp_collective/roi/stats_loom_latency_nan.csv")
View(stats_loom_latency_nan)
setwd("~/Documents/code/stats")
data <- read.csv("../../data/temp_collective/roi/stats_loom_latency_nan.csv",header=TRUE,na.strings=c("[nan]"))
lat1 <- data$latency
temp <- stats_loom_latency$Temperature
gs <- stats_loom_latency$Groupsize
loom <- stats_loom_latency$loom
n <- length(lat1)
lat <- data$latency
model1 <- lm(lat ~ temp + gs,data)
summary(model1)
temp <- data$Temperature
gs <- data$Groupsize
loom <- data$loom
n <- length(lat1)
model1 <- lm(lat ~ temp + gs,data)
summary(model1)
library(readr)
stats_loom_latency_nan <- read_csv("~/Documents/data/temp_collective/roi/stats_loom_latency_nan.csv")
View(stats_loom_latency_nan)
setwd("~/Documents/code/stats")
data <- read.csv("../../data/temp_collective/roi/stats_loom_latency_nan.csv",header=TRUE,na.strings=c("[nan]"))
lat <- data$latency
temp <- data$Temperature
gs <- data$Groupsize
loom <- data$loom
n <- length(lat)
model1 <- lm(lat ~ temp + gs,data)
summary(model1)
model2 <- lm(lat ~ temp*gs,data)
summary(model1)
summary(model2)
model3 <- lm(lat ~ temp + gs + loom,data)
summary(model3)
model4 <- lm(lat ~ temp*gs + loom,data)
summary(model4)
model5 <- lm(lat ~ temp+gs + loom+I(temp^2),data)
summary(model5)
model6 <- lm(lat ~ temp*gs + loom+I(temp^2),data)
summary(model6)
model7 <- lm(lat ~ temp*loom + gs +I(temp^2),data)
summary(model7)
model8 <- lm(lat ~ temp*loom*gs +I(temp^2),data)
summary(model8)
model8 <- lm(lat ~ loom*gs + temp + I(temp^2),data)
summary(model8)
summary(model8)
plot(fitted(model8), residuals(model8))
qqnorm(residuals(model8))
qqline(residuals(model8))
model9 <- lm(lat ~ temp*loom*gs +I(temp^2),data)
summary(model9)
plot(fitted(model9), residuals(model9))
qqnorm(residuals(model9))
qqline(residuals(model9))
model10 <- lm(lat ~ temp*loom*gs*I(temp^2),data)
summary(model10)
plot(fitted(model10), residuals(model10))
qqnorm(residuals(model10))
qqline(residuals(model10))
lat
library(readr)
startles_ratio_csv <- read_csv("~/Documents/data/temp_collective/roi/startles_ratio_csv.csv")
View(startles_ratio_csv)
data <- read.csv("../../data/temp_collective/roi/startles_ration_csv.csv",header=TRUE,na.strings=c("[nan]"))
data <- read.csv("../../data/temp_collective/roi/startles_ratio_csv.csv",header=TRUE,na.strings=c("[nan]"))
accuracy <- data$accuracy
temp <- data$Temperature
gs <- data$Groupsize
date <- data$Date
date
time_in <- data$Time_fish_in
time_record <- data$Time_start_record
time_in[1]
time_in[0]
time_record[1]
time_record[1] - time_in[1]
as.Date(time_in[1], format = "%m/%d/%Y")
as.POSIXct("25072013 08:32:07", format = "%d%m%Y %H:%M:%S")
as.POSIXct(time_in[1], format = "%H:%M")
as.POSIXct(time_record[1], format = "%H:%M") - as.POSIXct(time_in[1], format = "%H:%M")
a = as.POSIXct(time_record[1], format = "%H:%M") - as.POSIXct(time_in[1], format = "%H:%M")
a
date[1]
as.Date(date[1], format = "%m/%d/%Y")
as.Date(date[1], format = "%d/%m/%Y")
date <- as.Date(data$Date, format = "%d/%m/%Y")
time_in <- as.POSIXct(data$Time_fish_in, format = "%H:%M")
time_record <- as.POSIXct(data$Time_start_record, format = "%H:%M")
date[1] - date[2]
date[1]
as.numeric(date[1])
date[3]
as.numeric(date[3])
date <- as.numeric(as.Date(data$Date, format = "%d/%m/%Y"))
date
time_in[1]
as.numeric(time_in[1])
as.numeric(time_record[1])
as.numeric(time_record[1]) - as.numeric(time_in[1])
as.numeric(time_record[2]) - as.numeric(time_in[2])
as.numeric(time_record[10]) - as.numeric(time_in[10])
time_in <- as.numeric(as.POSIXct(data$Time_fish_in, format = "%H:%M"))
time_record <- as.numeric(as.POSIXct(data$Time_start_record, format = "%H:%M"))
difference <- time_record - time_in
difference
library(reticulate)
np <- import("numpy")
install.packages("reticulate")
library(reticulate)
np <- import("numpy")
n
mat <- np$load("fmat.npy")
getwd()
setwd("~/Documents/data/temp_collective/roi/13/1/GS_1_T_13_roi_1")
mat <- np$load("trajectories.npy")
mat <- np$load("trajectories.npy",allow_pickle = True)
mat <- np$load("trajectories.npy",allow_pickle = TRUE)
mat
mat[1]
mat.shape
mat[1][1]
View(mat)
mat[[1]][["trajectories"]]
mat[[1]][["trajectories"]][1]
mat[[1]][["trajectories"]][1][1]
mat[[1]][["trajectories"]][1][2]
mat[[1]][["trajectories"]][0]
mat[[1]][["trajectories"]][2]
mat[[1]][["trajectories"]][9544]
shape()
length(mat[[1]][["trajectories"]])
97119*2
mat[[1]][["trajectories"]][194238]
model1 <- lm(accuracy ~ temp + gs,data)
summary(model1)
model2 <- lm(accuracy ~ temp + gs + I(temp^2),data)
summary(model2)
plot(fitted(model1), residuals(model1))
qqnorm(residuals(model1))
qqline(residuals(model1))
require(MASS)
boxcox(model1,plotit=TRUE)
accuracy
model3 <- lm(accuracy ~ temp + gs + date + difference,data)
summary(model3)
plot(fitted(model3), residuals(model3))
qqnorm(residuals(model3))
qqline(residuals(model3))
plot(fitted(model3)^2, residuals(model3))
plot(fitted(model3), residuals(model3)^2)
library(readr)
startles_ratio_zero <- read_csv("~/Documents/data/temp_collective/roi/startles_ratio_zero.csv")
View(startles_ratio_zero)
data1 <- read.csv("../../data/temp_collective/roi/startles_ratio_zero.csv",header=TRUE,na.strings=c("[nan]"))
accuracy1 <- data1$accuracy
temp1 <- data1$Temperature
gs1 <- data1$Groupsize
date1 <- as.numeric(as.Date(data1$Date, format = "%d/%m/%Y"))
time_in1 <- as.numeric(as.POSIXct(data1$Time_fish_in, format = "%H:%M"))
time_record1 <- as.numeric(as.POSIXct(data1$Time_start_record, format = "%H:%M"))
difference1 <- time_record - time_in
setwd("~/Documents/data/temp_collective/roi")
library(readr)
startles_ratio_zero <- read_csv("~/Documents/data/temp_collective/roi/startles_ratio_zero.csv")
View(startles_ratio_zero)
data1 <- read.csv("../../data/temp_collective/roi/startles_ratio_zero.csv",header=TRUE,na.strings=c("[nan]"))
accuracy1 <- data1$accuracy
temp1 <- data1$Temperature
gs1 <- data1$Groupsize
date1 <- as.numeric(as.Date(data1$Date, format = "%d/%m/%Y"))
time_in1 <- as.numeric(as.POSIXct(data1$Time_fish_in, format = "%H:%M"))
time_record1 <- as.numeric(as.POSIXct(data1$Time_start_record, format = "%H:%M"))
difference1 <- time_record - time_in
setwd("~/Documents/code/stats")
data1 <- read.csv("../../data/temp_collective/roi/startles_ratio_zero.csv",header=TRUE,na.strings=c("[nan]"))
accuracy1 <- data1$accuracy
temp1 <- data1$Temperature
gs1 <- data1$Groupsize
date1 <- as.numeric(as.Date(data1$Date, format = "%d/%m/%Y"))
time_in1 <- as.numeric(as.POSIXct(data1$Time_fish_in, format = "%H:%M"))
time_record1 <- as.numeric(as.POSIXct(data1$Time_start_record, format = "%H:%M"))
difference1 <- time_record - time_in
model4 <- lm(accuracy1 ~ temp1 + gs1,data1)
summary(model4)
plot(fitted(model4), residuals(model4))
qqnorm(residuals(model4))
qqline(residuals(model4))
require(MASS)
boxcox(model4,plotit=TRUE)
model5 <- lm(accuracy1^(3/2) ~ temp1 + gs1,data1)
summary(model5)
plot(fitted(model5), residuals(model5))
qqnorm(residuals(model5))
qqline(residuals(model5))
model6 <- lm(accuracy^(3/2) ~ temp + gs,data)
summary(model6)
plot(fitted(model6), residuals(model6))
qqnorm(residuals(model6))
qqline(residuals(model6))
model5 <- lm(I(accuracy1^(3/2)) ~ temp1 + gs1,data1)
summary(model5)
model7 <- lm(accuracy ~ temp*gs,data)
summary(model7)
plot(fitted(model7), residuals(model7))
qqnorm(residuals(model7))
qqline(residuals(model7))
model8 <- lm(accuracy1 ~ temp1*gs1,data1)
summary(model8)
plot(fitted(model8), residuals(model8))
qqnorm(residuals(model8))
qqline(residuals(model8))
require(MASS)
boxcox(model8,plotit=TRUE)
model9 <- lm(accuracy1^(3/2) ~ temp1*gs1,data1)
summary(model9)
plot(fitted(model9), residuals(model9))
qqnorm(residuals(model9))
qqline(residuals(model9))
mat[[1]][["trajectories"]]
mat[[1]][["trajectories"]][0]
mat[[1]][["trajectories"]][9544]
mat[[1]][["trajectories"]][10]
mat[[1]][["trajectories"]][1]
mat[[1]][["trajectories"]][1][1]
mat[[1]][["trajectories"]][1][2]
mat[[1]][["trajectories"]][1][2]
mat[[1]][["trajectories"]][1][23
]
mat[[1]][["trajectories"]][9544,1]
mat[[0]][["trajectories"]][9544,1]
mat[[2]][["trajectories"]][9544]
library(readr)
GS_1_T_13_rep_1 <- read_csv("~/Documents/data/temp_collective/csv/GS_1_T_13_rep_1.csv")
View(GS_1_T_13_rep_1)
data_csv <- read.csv(GS_1_T_13_rep_1)
data_csv <- read.csv("../../data/temp_collective/csv/GS_1_T_13_rep_1.csv")
data_csv$acceleration
