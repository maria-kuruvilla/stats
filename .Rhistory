# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:ndata){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
return(Nt)
return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
#  Below I have sketched out code for you.  Code comments marked with ### indicate where you
#  have to write your own code, following the instructions
# The data:
Ntobs <- c(2570,
NA,
2864,
4408,
5197,
4416,
4203,
6008,
4807,
7600,
6796,
6475,
NA,
8681,
7761,
8161,
5786,
6492,
7191,
7634,
NA,
7117)
years <- 1978:1999
# a function for caclulating negative log likelihood for density independent model
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:ndata){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
#return(Nt)
#return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
# fit density independent model
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
### use optim to get smallest negative log likelihood from density independent model
ddind.nll <- ddind.soln$value # negative log likelihood of density independebnt model
ddind.soln
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
ddind.soln
density.independent.nll(start.pars,Ntobs)
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
return(Nt)
return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
density.independent.nll(start.pars,Ntobs)
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
# The data:
Ntobs <- c(2570,
NA,
2864,
4408,
5197,
4416,
4203,
6008,
4807,
7600,
6796,
6475,
NA,
8681,
7761,
8161,
5786,
6492,
7191,
7634,
NA,
7117)
years <- 1978:1999
# a function for caclulating negative log likelihood for density independent model
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
#return(Nt)
#return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
ddind.soln
# The data:
Ntobs <- c(2570,
NA,
2864,
4408,
5197,
4416,
4203,
6008,
4807,
7600,
6796,
6475,
NA,
8681,
7761,
8161,
5786,
6492,
7191,
7634,
NA,
7117)
years <- 1978:1999
# a function for caclulating negative log likelihood for density independent model
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
#return(Nt)
#return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
ddind.soln
# a function for caclulating negative log likelihood for density independent model
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
return(Nt)
return(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
density.independent.nll(start.pars,Ntobs)
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
print(Nt)
print(epsilon_t)
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
density.independent.nll(start.pars,Ntobs)
mean(epsilon_t)
# a function for caclulating negative log likelihood for density independent model
density.independent.nll <- function(pars, Ntobs) {
r <- pars[1]
N1978 <- pars[2]
sigma <- pars[3]
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
### write code to estimate Nt for all years
### then calculate epsilon_t
### then caclulate the sum of the negative log likelihood, removing NA's:
# e.g.: - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE)
### make sure the function returns the sum of negative log likelihoods
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
epsilon_t <- Ntobs - Nt
print(Nt)
print(mean(epsilon_t))
return( - sum(dnorm(x = epsilon_t, mean = 0, sd = sigma, log = T), na.rm = TRUE))
}
start.pars <- c(0.1, 3000, 400) # these seem to "work"
ddind.soln <- optim(start.pars, density.independent.nll, method = "BFGS", Ntobs = Ntobs)
ddind.soln
Nt[1] <- N1978
N1978 <- pars[2]
start.pars <- c(0.1, 3000, 400)
N1978 <- start.pars[2]
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
ndata <- length(Ntobs)
Nt <- rep(NA, ndata)
epsilon_t <- rep(NA, ndata)
Nt[1] <- N1978
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
r <- start.pars[1]
for(i in 1:(ndata-1)){
Nt[i+1] <- Nt[i] + r* Nt[i]
}
Nt
epsilon_t <- Ntobs - Nt
epsilon_t
install.packages("xtable")
library(readr)
stats_speed <- read_csv("Documents/data/temp_collective/roi/stats_speed.csv")
View(stats_speed)
speed <- stats_speed$`90_speed`
temp <- stats_speed$Temperature
gs <- stats_speed$Groupsize
n <- length(stats_speed$`90_speed`)
speed_data <- as.data.frame(cbind(sample = (1:n),speed,temp,gs))
model <- lm(speed ~ temp + gs,speed_data)
model$coefficients
plot(fitted(model),model$residuals)
plot(fitted(model),sqrt(abs(model$residuals)))
qqnorm(residuals(model))
qqline(residuals(model))
plot.default(temp,model$residuals)
plot.default(gs,model$residuals)
plot.default(temp,model$residuals)
plot(fitted(model),model$residuals)
shapiro.test(residuals(model))
install.packages(c("faraway", "lessR"))
summary(model)
boxcox(model, plotit = TRUE)
install.packages("EnvStats")
boxcox(model, plotit = TRUE)
require(EnvStats)
boxcox(model, plotit = TRUE)
boxcox(model, plotit = TRUE)
install.packages("MASS")
require(MASS)
boxcox(model, plotit = TRUE)
boxcox(model, plotit = FALSE)
10^(0.1)
model_transform <- lm(speed^0.1 ~ temp + gs,speed_data)
summary(model_transform)
plot(fitted(model_transform), residuals(model_transform))
qqnorm(residuals(model_transform))
qqline(residuals(model_transform))
ln(1)
log(1)
log(10)
log(2.7)
model_exp <- lm(log(speed) ~ temp + gs,speed_data)
summary(model_exp)
plot(fitted(model_exp), residuals(model_exp))
qqnorm(residuals(model_exp))
qqline(residuals(model_exp))
plot(fitted(model_exp), residuals(model_exp))
plot(fitted(model), residuals(model))
shapiro.test(residuals(model_exp))
exp(2)
transform.bc <- function(x)(exp(x))
data.frame(x = temp)
data.frame(x = temp + gs)
summary(model_exp)
transform.bc <- function(x)(exp(x))
out <- predict(lmod.t, newdata = data.frame(x = temp), interval="prediction")
transform.bc <- function(x)(exp(x))
out <- predict(model_exp, newdata = data.frame(x = temp), interval="prediction")
plot(speed ~ temp, ylab = "90th percentile of speed", xlab = "Temperature")
lines(sort(temp),sort(transform.bc(out)[,1] ) ,col="red")
lines(sort(temp),sort(transform.bc(out)[,2] ),lty = 2,col="red")
plot(speed ~ temp, ylab = "90th percentile of speed", xlab = "Temperature")
lines(sort(temp),sort(transform.bc(out)[,1] ) ,col="red")
lines(sort(temp),sort(transform.bc(out)[,2] ),lty = 2,col="red")
lines(sort(temp),sort(transform.bc(out)[,3] ),lty = 2,col="red")
seq(9, 29, by=2)
data.frame(x = seq(9,29,by = 2))
transform.bc <- function(x)(exp(x))
out <- predict(model_exp, newdata = data.frame(x =seq(9,29,by = 2)), interval="prediction")
plot(speed ~ temp, ylab = "90th percentile of speed", xlab = "Temperature")
lines(sort(temp),sort(transform.bc(out)[,1] ) ,col="red")
lines(sort(temp),sort(transform.bc(out)[,2] ),lty = 2,col="red")
lines(sort(temp),sort(transform.bc(out)[,3] ),lty = 2,col="red")
lines(seq(9,29,by = 2),sort(transform.bc(out)[,1] ) ,col="red")
lines(seq(9,29,by = 2),sort(transform.bc(out)[,2] ),lty = 2,col="red")
lines(seq(9,29,by = 2),sort(transform.bc(out)[,3] ),lty = 2,col="red")
sort(transform.bc(out)[,1]
a
transform.bc(out)
out <- predict(model_exp, newdata = data.frame(x =seq(9,29,by = 2)), interval="prediction")
transform.bc(out)
out2 <- predict(model_exp, newdata = data.frame(x = gs), interval="prediction")
plot(speed ~ gs, ylab = "90th percentile of speed", xlab = "Group Size")
lines(sort(gs),sort(transform.bc(out2)[,1] ) ,col="red")
lines(sort(gs),sort(transform.bc(out2)[,2] ),lty = 2,col="red")
lines(sort(gs),sort(transform.bc(out2)[,3] ),lty = 2,col="red")
Correlation(temp,gs)
install.packages("lessR")
require(lessR)
install.packages("latticeExtra"")
a
adf
f
s
vswrdvw
dsvsvs
speeed
speed
h <- hatvalues(model_exp)
two_p_over_n <- 2*mean(h)
speed_data$sample[which(h>two_p_over_n)]
num <- speed_data$sample
halfnorm(h,nlab = 5, labs= num, ylab="Leverages")
require(faraway)
install.packages("faraway")
require(faraway)
install.packages("lme4")
stats_speed <- read.csv("../../data/temp_collective/roi/stats_speed.csv",header=TRUE,na.strings=c("[nan]"))
speed <- stats_speed$X90_speed
temp <- stats_speed$Temperature
gs <- stats_speed$Groupsize
n <- length(stats_speed$X90_speed)
speed_data <- as.data.frame(cbind(sample = (1:n),speed,temp,gs))
library(readr)
stats_speed <- read_csv("Documents/code/stats/stats_speed.csv")
View(stats_speed)
stats_speed <- read.csv("../../data/temp_collective/roi/stats_speed.csv",header=TRUE,na.strings=c("[nan]"))
speed <- stats_speed$X90_speed
temp <- stats_speed$Temperature
gs <- stats_speed$Groupsize
n <- length(stats_speed$X90_speed)
speed_data <- as.data.frame(cbind(sample = (1:n),speed,temp,gs))
stats_speed <- read.csv("../../data/temp_collective/roi/stats_speed.csv",header=TRUE,na.strings=c("[nan]"))
speed <- stats_speed$'90_speed'
temp <- stats_speed$Temperature
gs <- stats_speed$Groupsize
n <- length(stats_speed$X90_speed)
speed_data <- as.data.frame(cbind(sample = (1:n),speed,temp,gs))
n <- length(stats_speed$'90_speed')
speed_data <- as.data.frame(cbind(sample = (1:n),speed,temp,gs))
model_exp <- lm(log(speed) ~ temp + log(gs,2),speed_data)
summary(model_exp)
model_exp_inv <- lm(log(speed) ~ temp*gs,speed_data)
summary(model_exp_inv)
model_log <- lm(log(speed) ~ temp + I(temp^2) + log(gs,2),speed_data)
summary(model_log)
xtable(model_exp_inv)
require(xtable)
xtable(model_exp_inv)
stats_speed_acc_latency <- read.csv("../../data/temp_collective/roi/stats_speed_acc_latency.csv",header=TRUE,na.strings=c("[nan]"))
acc <- stats_speed_acc_latency$X90_acc
lat <- stats_speed_acc_latency$latency
temp <- stats_speed_acc_latency$Temperature
gs <- stats_speed_acc_latency$Groupsize
n <- length(stats_speed_acc_latency$`90_acc`)
data <- as.data.frame(cbind(sample = (1:n),temp,gs,acc,lat))
setwd("~/Documents/code/stats")
stats_speed_acc_latency <- read.csv("../../data/temp_collective/roi/stats_speed_acc_latency.csv",header=TRUE,na.strings=c("[nan]"))
acc <- stats_speed_acc_latency$X90_acc
lat <- stats_speed_acc_latency$latency
temp <- stats_speed_acc_latency$Temperature
gs <- stats_speed_acc_latency$Groupsize
n <- length(stats_speed_acc_latency$`90_acc`)
data <- as.data.frame(cbind(sample = (1:n),temp,gs,acc,lat))
n <- length(stats_speed_acc_latency$X90_acc)
data <- as.data.frame(cbind(sample = (1:n),temp,gs,acc,lat))
model_acc <- lm(log(acc) ~ temp + gs,data)
summary(model_acc)
plot(fitted(model_acc), residuals(model_acc))
qqnorm(residuals(model_acc))
qqline(residuals(model_acc))
shapiro.test(residuals(model_acc))
model_acc_int <- lm(log(acc) ~ temp*gs,data)
summary(model_acc_int)
plot(fitted(model_acc_int), residuals(model_acc_int))
qqnorm(residuals(model_acc_int))
qqline(residuals(model_acc_int))
shapiro.test(residuals(model_acc_int))
model_acc_int_quad <- lm(log(acc) ~ temp*gs + I(temp^2),data)
summary(model_acc_int_quad)
plot(fitted(model_acc_int_quad), residuals(model_acc_int_quad))
qqnorm(residuals(model_acc_int_quad))
qqline(residuals(model_acc_int_quad))
shapiro.test(residuals(model_acc_int))
model_acc_int_quad <- lm(log(acc) ~ temp*gs*I(temp^2),data)
summary(model_acc_int_quad)
model_acc_int_quad <- lm(log(acc) ~ temp*gs + I(temp^2),data)
summary(model_acc_int_quad)
library(readr)
stats_speed_acc_latency <- read_csv("stats_speed_acc_latency.csv")
View(stats_speed_acc_latency)
acc <- stats_speed_acc_latency$`90_acc`
lat <- stats_speed_acc_latency$latency
temp <- stats_speed_acc_latency$Temperature
gs <- stats_speed_acc_latency$Groupsize
n <- length(stats_speed_acc_latency$latency)
data <- as.data.frame(cbind(sample = (1:n),temp,gs,acc,lat))
model_lat <- lm(lat ~ temp + gs,data)
summary(model_lat)
setwd("~/Documents/code/stats")
data <- read.csv("../../data/temp_collective/roi/stats_loom_latency_nan.csv",header=TRUE,na.strings=c("[nan]"))
lat <- data$latency
#lat <- as.numeric(lat1)
temp <- data$Temperature
gs <- data$Groupsize
loom <- data$loom
n <- length(lat)
model_pois2 <- glm(lat ~ temp*gs, family = quasipoisson, data)
summary(model_pois2)
model_pois <- glm(lat ~ temp + gs, family = quasipoisson, data)
summary(model_pois)
model_pois3 <- glm.nb(lat ~ temp*gs, data)
summary(model_pois3)
model_pois <- glm(lat ~ temp + gs, family = quasipoisson, data)
summary(model_pois)
model_pois <- glm(lat ~ temp + gs + I(temp^2), family = quasipoisson, data)
summary(model_pois)
data <- read.csv("../../data/temp_collective/roi/startles_ratio_csv.csv",header=TRUE,na.strings=c("[nan]"))
accuracy <- data$accuracy
hist(accuracy)
temp <- data$Temperature
gs <- data$Groupsize
date <- as.numeric(as.Date(data$Date, format = "%d/%m/%Y"))
time_in <- as.numeric(as.POSIXct(data$Time_fish_in, format = "%H:%M"))
time_record <- as.numeric(as.POSIXct(data$Time_start_record, format = "%H:%M"))
difference <- time_record - time_in
model1 <- lm(accuracy ~ temp + gs,data)
summary(model1)
model11 <- glm(accuracy ~ temp*gs, family = binomial,data)
summary(model11)
model10 <- glm(accuracy ~ temp + gs, family = binomial,data)
summary(model10)
model11 <- glm(accuracy ~ temp*gs, family = binomial,data)
summary(model11)
model10 <- glm(accuracy ~ temp + gs + I(temp^2), family = binomial,data)
summary(model10)
model12 <- glm(accuracy ~ temp + gs, family = binomial,data)
summary(model12)
setwd("~/Documents/code/stats")
library(readr)
stats_startles_ratio_new_mask <- read_csv("~/Documents/data/temp_collective/roi/stats_startles_ratio_new_mask.csv")
View(stats_startles_ratio_new_mask)
